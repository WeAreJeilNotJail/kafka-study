# Chapter 3
## 카프카 기본 개념 설명

### 3-1. 카프카 브로커, 클러스터, 주키퍼

주키퍼는 클라이언트와 메시지를 주고 받는 주체이다.

서버 한대로도 기능 실행되지만, 죽는 경우를 대비해 3대 이상을 띄우고 하나의 클러스터로 묶는다.

이들은 데이터를 분산 저장하고 복제하는 역할을 한다.

- __```데이터 저장, 전송```__

브로커는 프로듀서가 요청한 토픽의 파티션에 데이터 저장하고 컨슈머가 요청하면 해당 데이터를 전달하고 
프로듀서로부터 받은 데이터는 파일 시스템에 저장된다.

```
{topic}-{topic}의 이름으로 저장 (ex. hello.kafka-0)
.index .log .timeindex 등 존재
- log: 메타 데이터
- index: 메시지의 오프셋 인덱싱한 정보
- timeindex: 메시지에 포함된 timestamp 값을 기준으로 인덱싱한 정보 
```

카프카는 메모리, db를 이용하지 않고 파일 시스템에 저장하지만 ```페이지 캐시```를 사용하여 디스크 입출력 속도를 높인다.

*페이지 캐시*

OS에서 파일 입출력의 성능 향상을 위해 만든 메모리 영역으로 추후 동일한 파일 접근이 있을 때,
파일에서 읽지 않고 메모리에서 읽는다.

- __```데이터 복제, 싱크```__

클러스터 내의 브로커 중 일부가 죽어도 나머지가 동작해 데이터를 유실하지 않고 안전하게 사용 가능하다.

데이터 복제는 파티션 단위로 이루어진다. (파티션은 리더 + 팔로워로 구성)

```
리더: 프로듀서, 컨슈머와 직접 통신하는 파티션
팔로워: 나머지 복제 데이터를 갖는 파티션
```

팔로워는 리더의 오프셋을 확인해서 차이가 나면 데이터를 가져와 자신의 파티션에 저장하게 되는데 이를 "복제"라고 부른다.

- __```컨트롤러```__

클러스터의 브로커 중 한대가 컨트롤러의 역할을 하며, 다른 브로커들의 상태를 체크하고 클러스터에서 브로커가 빠지는 경우 리더 파티션을 재분배한다.

- __```데이터 삭제```__

컨슈머가 데이터를 가져가도 토픽의 데이터는 삭제되지 않는다. 오직 브로커만이 삭제할 수 있고, 파일 단위로 이루어진다. (로그 세그먼트)

데이터가 쌓이는 동안 파일 시스템으로 열려 있으며 1GB(기본값) 용량에 도달하면 닫히게 되고 log.retention.bytes, log.retention.ms 설정값을 넘으면 삭제된다.

- __```컨슈머 오프셋 저장```__

컨슈머 그룹은 어디까지 데이터를 읽었는지 알기 위해 오프셋을 커밋하고 저장한다.

- __```코디네이터```__

클러스터의 브로커 중 한대가 담당하며, 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배한다.

***

### 3-2. 토픽과 파티션

토픽은 데이터를 구분하는 단위이며 1개 이상의 파티션을 갖는다.

파티션에는 프로듀서가 보낸 데이터가 담기는데 이를 레코드라고 한다. (큐와 비슷)

컨슈머에서 데이터를 가져가도 사라지지 않기 때문에 여러 컨슈머 그룹들이 여러번 가져갈 수 있다.

***

### 3-3. 레코드

타임스탬프, 메시지 키, 메시지 값, 오프셋, 헤더로 구성된다.

동일한 메시지 키는 동일 파티션에 들어가게 된다. (파티션 갯수가 변경되면 메시지 키, 파티션 매칭이 달라짐)
-> 메시지 키를 설정하지 않으면 (null) 프로듀서 기본 설정 파티셔너에 따라 분배

메시지 키, 값은 직렬화해서 전달되기 때문에 동일한 형태로 컨슈머가 역직렬화 해야 한다.

헤더는 레코드의 추가 정보를 담는 메타데이터 저장소이다.

***

### 3-4. 카프카 클라이언트

__```프로듀서```__

프로듀서 애플리케이션은 데이터를 선언하고 브로커의 특정 파티션에 전송하는데 리더 파티션을 갖고 있는 브로커와 통신한다.

프로듀서는 데이터를 전송할 때, 파티셔너 -> 배치 생성 단계를 거친다.

- 파티셔너: 토픽의 어떤 파티션으로 전송할 것인지를 결정
- 배치: 파티셔너로 구분된 레코드들은 어큐뮬레이터에 버퍼로 쌓아놓고 배치로 한번에 전송 (처리량 향상)

__```컨슈머```__

컨슈머 그룹으로 묶인 컨슈머들은 토픽의 1개 이상 파티션에서 데이터를 가져갈 수 있다.

1개의 파티션은 최대 1개의 컨슈머에 할당되는데, 파티션의 갯수 < 컨슈머의 갯수가 되면 놀고 있는 컨슈머가 생긴다.

```
파티션의 갯수 >= 컨슈머의 갯수
```

컨슈머 그룹 중 일부에 장애가 발생하면 해당 컨슈머에 할당된 파티션은 다른 컨슈머에 할당되는데 이를 **리밸런싱**이라고 한다.

__```오프셋 커밋```__

컨슈머는 브로커로부터 읽은 데이터의 정보를 **커밋**을 통해 기록한다.

어떤 컨슈머 그룹이 특정 토픽의 파티션을 몇번째까지 가져갔는지 **브로커**의 내부 토픽에 기록
-> 해당 과정이 제대로 되지 않으면 데이터 중복이 발생할 가능성이 있음

- commitSync: 동기 오프셋 커밋

poll() 메서드로 받은 마지막 레코드의 오프셋을 기준으로 커밋한다. 동기 커밋의 경우 완료되기까지 기다려야 하므로 처리량이 적다.

- commitAsync: 비동기 오프셋 커밋

poll() 메서드로 받은 마지막 레코드의 오프셋을 기준으로 커밋하지만 커밋이 완료될때까지 응답을 기다리지 않아 처리량이 더 많다.

### 3-5. 카프카 스트림즈

```Stream```을 기반으로 토픽에 저장된 데이터를 **실시간**으로 처리하는 라이브러리이다.

스트림을 기반으로 하는 애플리케이션은 **스레드**를 1개 이상 갖고 하나의 스레드에는 1개 이상의 **테스크**를 갖는다.

*(테스크: 데이터 처리 최소 단위)*

토픽의 파티션의 개수만큼 테스크가 할당되며 이로 인해 병렬로 처리하여 데이터 처리량을 늘릴 수 있다.

스트림에서 사용하는 토폴로지의 형태는 트리 형태와 유사하다.

*(토폴로지: 2개 이상의 노드와 선으로 이루어진 집합)*

- 노드: 프로세서
  - 소스 프로세서: 하나 이상의 토픽에서 데이터를 가져오는 역할
  - 스트림 프로세서: 다른 프로세서가 반환한 데이터를 처리하는 역할
  - 싱크 프로세서: 데이터를 카프카 토픽에 저장하는 역할
- 선: 스트림 (토픽의 데이터를 뜻하며 레코드와 동일)

__```스트림즈 DSL```__

- **KStream**

레코드의 흐름을 표현한 것으로 메시지 키-값으로 구성된다. 데이터 조회시, 해당 토픽에 존재하는 모든 레코드 출력된다.

- **KTable**

메시지 키를 기준으로 묶어서 사용하기 때문에 동일한 메시지 키를 갖는 경우 업데이트 된다고 볼 수 있다.

```
record                  KTable
{ "foo": 1 }     |     foo - 1
{ "test": 2 }    |  foo - 1, test - 2
{ "foo": 2 }     |  foo - 2, test - 2
```

- **GlobalKTable**

KTable과 동일하지만 KTable은 각 파티션의 데이터가 1개 테스크에 할당되지만 GlobalKTable은 모든 파티션 데이터가
각 테스크에 할당된다.

KStream과 KTable이 데이터 조인을 하려면 코파티셔닝이 되어 있어야 한다.

*코파티셔닝*

조인을 하는 2개 데이터의 파티션 개수와 파티셔닝 전략을 동일하게 맞추는 작업이다.
이 때, 동일한 메시지 키를 가진 데이터는 동일한 테스크에 들어가는 것을 보장할 수 있어 테스크는 조인을 진행할 수 있다.

*리파티셔닝*

이를 만족하지 않으면 리파티셔닝을 해야 하는데 해당 과정은 기존 데이터의 중복 생성과 파티션 재배열을 위한 프로세싱도 거치게 된다.
이 때, GlobalKTable로 선언하면 해결 가능하다. (GlobalKTable은 파티션의 데이터가 모든 테스크에 공유되므로)

각 테스크마다 모든 파티션의 데이터를 저장하므로 로컬 스토리지의 사용량이 증가한다는 단점이 존재한다.

***

### 3-6. 카프카 커넥트

데이터 파이프라인 생성시 반복 작업을 줄이고, 효율적인 전송을 위한 애플리케이션이다.

특정한 작업 형태를 템플릿으로 만든 커넥터를 실행하면 프로듀서 - 컨슈머를 만드는 등의 반복행위를 줄일 수 있다.

프로듀서 역할을 하는 ```소스 커넥터```와 컨슈머 역할을 하는 ```싱크 커넥터```로 나뉜다.

**단일 모드 커넥트**

단일 애플리케이션으로 실행되므로 1개의 프로세스만 사용한다. SPOF 발생의 여지가 있기에 개발환경이나 중요도가 낮은 파이프라인에 구성한다.

```connect-standalone.properties``` 파일을 통해 설정할 수 있다.

**분산 모드 커넥트**

2대 이상의 서버에 클러스터링하므로 하나의 서버가 죽어도 지속적으로 운영할 수 있다.

```connect-distributed.propeties``` 파일을 통해 설정할 수 있다.

__```소스 커넥터```__

소스(Application, mongoDB, S3, File, ..)로부터 데이터를 가져와 토픽으로 넣는 역할을 한다.

```
소스 -> 카프카 커넥트 (소스 커넥터의 소스 테스크) -> 카프카 (토픽)
```

__```싱크 커넥터```__

토픽의 데이터를 타깃 (application, mongoDB, S3, File, ...)로 저장하는 역할을 한다.

```
카프카 (토픽) -> 카프카 커넥트(싱크 커넥터의 싱크 테스크) -> 싱크
```

***

### 3-7. 카프카 미러메이커2

서로 다른 두 개의 카프카 클러스터 간에 토픽을 복제하는 애플리케이션이다.
프로듀서, 컨슈머를 이용해 데이터를 완전히 동일하게 옮기는 작업은 쉽지 않다. (동일한 토픽의 파티션 개수 + 파티셔너에 대한 정보 등)

- __```액티브-스탠바이 클러스터```__

서비스 애플리케이션과 통신하는 카프카 클러스터(*액티브 클러스터*) 외에 재해 복구를 위해 임시 클러스터(*스탠바이 클러스터*)를 더 구성하는 경우, 액티브-스탠바이 클러스터로 운영할 수 있다.

```
서비스 애플리케이션 -> 카프카 클러스터(액티브) ---미러 메이커2---> 카프카 클러스터(스탠바이)
```

- __```액티브-액티브 클러스터```__

서비스 애플리케이션의 통신 지연을 최소화하기 위해 2개 이상의 클러스터가 서로 미러링하며 운영할 수 있다.
보통 글로벌 서비스를 운영할 때, 이용한다. 

한국과 미국에서 서비스를 운영하고 있고 클러스터가 한국에만 있다면 미국 유저는 통신 지연이 발생할 수 있다.

```
서비스 애플리케이션 -> 카프카 클러스터(액티브) <---미러 메이커2---> 카프카 클러스터(액티브) <- 서비스 애플리케이션
```

- __```허브 앤 스포크 클러스터```__

소규모 카프카 클러스터 여러개를 중앙의 카프카 클러스터에 모아 데이터 레이크로 사용할 수 있다.
